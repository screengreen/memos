# RAG‑проект по книге 

## 1) Постановка задачи 
**Цель:** получать короткие, проверяемые ответы только из оцифрованной книги по переговорам/продажам/дейтингу.
- Агент использует информацию из предоставленных источников, а не из памяти
- Ответы без фантазий: если нет факта в тексте — писать «Не нашёл в памяти»
- Retrieveal умеет возвращять фрагменты, которые покрывают весь запрос, а не его часть
- в ответе были бы цитаты или очень похожие на оригинальные куски текста
- поддерживает русский и английский, а отвечает на языке вопроса

---

## 2) План vs факт 
| Категория | Как планировал | Что получилось |
|---|---|---|
| Факты/определения | Точно, со ссылками | ✔️ Стабильно | 
| Сравнения | Баланс 2+ фрагментов | ⚠️ Может быть однобоко, видит иногда только часть фрагментов | 
| Примеры| Прямые цитаты или около копия | ⚠️ Бывает перефраз | 
| «Нет в книге» | выдача "не знаю", если нет информации | ⚠️ Не всегда | 
| Стабильность, не мешающая качеству | Похожие ответы | ⚠️ Слегка плавает | 
|Мультиязычность| Есть| ✔️ Есть|
|ответ на языке вопроса| Есть| ⚠️ есть, но иногда путается в языках|

---

## 3) Результаты оценки retrieval (все метрики лежат в ноутбуке в выводе ячейки)
Для просчета метрик был сделан датасет на 550 вопросов и ответов, он лежит в qas.pkl

| Метод   | Hit@1 | Hit@5 | Hit@10 | Recall@5 | Recall@20 | MRR   | nDCG@5 | nDCG@20 |
|---------|-------|-------|--------|----------|-----------|-------|--------|---------|
| BM25    | 0.481 | 0.642 | 0.705  | 0.642    | **0.769** | 0.557 | 0.569  | 0.605   |
| Vec     | 0.329 | 0.547 | 0.663  | 0.547    | 0.764     | 0.439 | 0.449  | 0.512   |
| RRF     | 0.497 | 0.692 | 0.747  | 0.692    | **0.769** | 0.576 | 0.598  | 0.622   |
| Rerank  | **0.664** | **0.757** | **0.767** | **0.757** | **0.769** | **0.707** | **0.718** | **0.722** |

### Выводы по метрикам
- **BM25** даёт хорошие базовые метрики
- **Vec (векторный поиск)** обеспечивает близкий уровень полноты на больших k, но заметно хуже по точности в топ-1.  
- **RRF** объединяет BM25 и Vec: Recall@20 остаётся высоким (0.77), но Hit@1 чуть ниже, чем у BM25.  
- **Rerank (переранжирование)** даёт лучшие результаты по ключевым метрикам:  
  - Hit@1 = **0.66** (лучший топ-1)  
  - Recall@5 = **0.76** (лучшее покрытие в топ-5)  
  - nDCG@20 = **0.72** (лучшее качество ранжирования в глубину)  
  - MRR = **0.71**  

---

## 4) Гипотезы и улучшения

1) чтобы добавить больше около копий, добавить более стабильное "не знаю", повысить стабильность и сделать выбор языка ответа более точным:  
**Промпт и формат ответа** 
- Температуру снизить до **0.2–0.3**; добавить правило: «каждый пункт — с **цитатой**.  
- Улучшить информативность промпта, лучше оформить структуру подачи текста в модель. Применить все техники промпт
инжиниринга, тут думаю очень много чего можно добавить. 

2) Чтобы брать более разношерстные подходящие фрагменты:
- **MMR‑диверсификация** на топ‑N (например, N=50 -> 10–15 уникальных и непохожих кандидатов), чтобы сравнения подтягивали «обе стороны». (MMR (Maximal Marginal Relevance) выбирает кандидатов не только по релевантности к запросу, но и по **непохожести друг на друга**.  первый фрагмент — самый релевантный. Второй выбираем так, чтобы он и релевантный был, и **не повторял** первый. Дальше — то же правило.)
- может быть разбивать запрос на подзапроссы, для каждого проводить Retrieval, и брать из каждого по n/кол-во подзапроссов кандидатов

3) Улучшение reteiveal в целом (статьи на почитать, внедрить ):   
- **Pseudo‑Relevance Feedback (RM3/Bo1)** поверх BM25 для добавки терминов.
- **Doc2Query / GPL**: расширить индекс псевдо‑вопросами.
- **HyDE**: сгенерировать «гипотетический ответ» по вопросу и искать его эмбеддингом — хорошо работает на сюжетных вопросах и примерах диалогов.

4) Внедрение улучшений
- Безусловно мерить надо все по метрикам, их надо ввести и уже с их учетом внедрять изменения
- Провести подбор параметров retriveal, например размер чанков, overlap, 

Возможные метрики:
- **Retrieval (поиск):**
  - **Recall@k** (k=10, 50): доля вопросов, где «правильный» фрагмент попал в топ‑k.
  - **nDCG@10**: учитывает позиции нескольких релевантных фрагментов (чем выше — тем лучше).
  - **MRR@10**: средняя обратная позиция первого релевантного результата (ценит, когда нужный фрагмент высоко).
- **Реранк:**
  - **MAP** или **nDCG@10** до/после реранка — прирост означает, что порядок стал качественнее.
- **Генерация:**
  - **Attribution rate ≥ 95%**: доля утверждений с корректной ссылкой `(doc#chunk)` и короткой цитатой.
  - **Faithfulness ≥ 95%**: эксперт/авто‑проверка, что утверждения подтверждаются цитатой.
  - **Coverage ≥ 80%**: закрыты все ключевые аспекты вопроса (для сравнения — обе стороны).
  - **Correct “Не нашёл в книге.” ≥ 90%** на специально подготовленных вопросах без ответа.
- **Стабильность:**
  - Разброс по **nDCG/Recall** между перезапусками < 5% и одинаковые ответы (по ключевым фактам) ≥ 80%.
---

## 5) Кратко о пайплайне 
- **Индексация:** `RecursiveCharacterTextSplitter` 
- **Гибридный поиск:** mem Os search + BM25.
- **Слияние:** RRF 
- **Реранк:** `CrossEncoder("mmarco-mMiniLMv2-L12-H384-v1")` 
- **Генерация:** Mistral 
